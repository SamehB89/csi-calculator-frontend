"""
Extract data from 'Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ.xlsx' and convert to enhanced JSON format
for integration with AI Chat Wizard CSI Lookup system.
"""
import pandas as pd
import json
import re
from pathlib import Path

# File paths
EXCEL_FILE = r'd:\SUPERMANn\CSI_Project\Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ.xlsx'
OUTPUT_JSON = r'd:\SUPERMANn\CSI_Project\frontend\data\csi-lookup-database-enhanced.json'
EXISTING_JSON = r'd:\SUPERMANn\CSI_Project\frontend\data\csi-lookup-database.json'

def clean_text(text):
    """Clean and normalize text"""
    if pd.isna(text):
        return ""
    return str(text).strip()

def extract_activities(text):
    """Extract activities from text, splitting by common delimiters"""
    if not text:
        return []
    # Split by comma, semicolon, or newline
    activities = re.split(r'[,;ØŒ\n]', text)
    return [a.strip() for a in activities if a.strip()]

def parse_excel_data():
    """Parse the Excel file and extract structured data"""
    print("ğŸ“– Reading Excel file...")
    df = pd.read_excel(EXCEL_FILE, sheet_name='Sheet1', header=None)
    
    # Initialize data structure
    enhanced_items = []
    current_category = None
    current_category_id = None
    
    category_map = {
        'Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„Ø®Ø±Ø³Ø§Ù†Ø©': ('concrete', 'Concrete Works'),
        'Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„ØªØ±Ø§Ø¨ÙŠØ©': ('earthworks', 'Earthworks'),
        'Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„Ù†Ø²Ø­': ('dewatering', 'Dewatering'),
        'Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„Ø¹Ø²Ù„': ('waterproofing', 'Waterproofing'),
        'Ø§Ù„ØªØ´Ø·ÙŠØ¨Ø§Øª': ('finishes', 'Finishes'),
        'Ø£Ø¹Ù…Ø§Ù„ ØªÙƒÙ…ÙŠÙ„ÙŠØ©': ('supplementary', 'Supplementary Works'),
        'Ø§Ù„Ø³Ø¨Ø§ÙƒØ©': ('plumbing', 'Plumbing')
    }
    
    print("ğŸ” Parsing rows...")
    
    for idx, row in df.iterrows():
        col0 = clean_text(row[0]) if len(row) > 0 else ""
        col1 = clean_text(row[1]) if len(row) > 1 else ""
        col2 = clean_text(row[2]) if len(row) > 2 else ""
        col3 = clean_text(row[3]) if len(row) > 3 else ""
        
        # Skip empty rows
        if not col0:
            continue
            
        # Check for category headers
        for ar_cat, (cat_id, en_cat) in category_map.items():
            if ar_cat in col0 or f'Ø¬Ø¯ÙˆÙ„ {ar_cat}' in col0:
                current_category = ar_cat
                current_category_id = cat_id
                print(f"  ğŸ“ Category: {ar_cat}")
                break
        
        # Skip header rows and definitions
        if 'Ø§Ù„Ø¨Ù†Ø¯' in col0 or 'ØªØ¹Ø±ÙŠÙ' in col0 or 'Ù…Ù„Ø§Ø­Ø¸Ø© ØªÙ†ÙÙŠØ°ÙŠØ©' in col0 or 'Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…' in col0:
            continue
        if 'Ø¬Ø¯ÙˆÙ„' in col0:
            continue
            
        # Check if this is a data row (has CSI Division in col1)
        if col1 and ('Division' in col1 or re.match(r'\d{2}\s*\d{2}', col1)):
            # Extract item name (Arabic with possible English in parentheses)
            item_name_ar = col0
            item_name_en = ""
            
            # Try to extract English name from parentheses
            en_match = re.search(r'\(([^)]+)\)', col0)
            if en_match:
                item_name_en = en_match.group(1)
                item_name_ar = re.sub(r'\s*\([^)]+\)\s*', ' ', col0).strip()
            
            # Generate item key
            item_key = generate_item_key(item_name_ar, current_category_id)
            
            # Parse CSI division
            csi_division = col1
            
            # Parse activities
            activities = extract_activities(col2)
            
            # Parse unit and notes
            unit_notes = col3
            default_unit = extract_unit(unit_notes)
            notes = extract_notes(unit_notes)
            
            item = {
                "item_key": item_key,
                "item_name_ar": item_name_ar,
                "item_name_en": item_name_en if item_name_en else translate_item_name(item_name_ar),
                "category_id": current_category_id,
                "csi_section": csi_division,
                "typical_activities": activities[:5],  # Limit to 5 activities
                "default_unit": default_unit,
                "implementation_notes": notes,
                "synonyms_ar": generate_synonyms_ar(item_name_ar),
                "synonyms_en": generate_synonyms_en(item_name_en if item_name_en else "")
            }
            
            enhanced_items.append(item)
            print(f"    âœ… {item_name_ar[:40]}...")
    
    return enhanced_items

def generate_item_key(name_ar, category_id):
    """Generate a unique item key"""
    # Map common terms to keys
    key_map = {
        'Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù†ÙØµÙ„Ø©': 'FOOT_ISO',
        'Ù‚ÙˆØ§Ø¹Ø¯ Ø´Ø±ÙŠØ·ÙŠØ©': 'FOOT_STRIP',
        'Ù„Ø¨Ø´Ø©': 'RAFT',
        'Ø³Ù…Ù„Ø§Øª': 'TIE_BEAM',
        'ÙƒÙ…Ø±Ø§Øª Ø£Ø±Ø¶ÙŠØ©': 'TIE_BEAM',
        'Ø£Ø¹Ù…Ø¯Ø©': 'COLUMN',
        'Ø¨Ù„Ø§Ø·Ø©': 'SLAB',
        'Ø¨Ù„Ø§Ø·Ø§Øª': 'SLAB',
        'Ø´Ø¯Ø©': 'FORMWORK',
        'ØªØ³Ù„ÙŠØ­': 'REINF',
        'ØµØ¨': 'CAST',
        'ØªØ¬Ø±ÙŠÙ': 'TOPSOIL',
        'Ø­ÙØ±': 'EXCAV',
        'Ø³Ù†Ø¯': 'SHORING',
        'Ø±Ø¯Ù…': 'BACKFILL',
        'ØªØ³ÙˆÙŠØ©': 'GRADING',
        'Ù†Ø²Ø­ Ø³Ø·Ø­ÙŠ': 'SURF_DRAIN',
        'Ù†Ø²Ø­ Ø¬ÙˆÙÙŠ': 'DEWATER',
        'ØªØµØ±ÙŠÙ': 'STORM',
        'Ø¹Ø²Ù„ Ù‚ÙˆØ§Ø¹Ø¯': 'WP_FOUND',
        'Ø¹Ø²Ù„ Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª': 'WP_WET',
        'Ø¹Ø²Ù„ Ø§Ù„Ø£Ø³Ø·Ø­': 'WP_ROOF',
        'Ø¹Ø²Ù„ Ø®Ø²Ø§Ù†Ø§Øª': 'WP_TANK',
        'Ù…Ø­Ø§Ø±Ø©': 'PLASTER',
        'Ø¯Ù‡Ø§Ù†Ø§Øª': 'PAINT',
        'Ø¨Ù„Ø§Ø·': 'TILE',
        'Ø±Ø®Ø§Ù…': 'STONE',
        'Ø¬Ø±Ø§Ù†ÙŠØª': 'STONE',
        'Ø£Ø±Ø¶ÙŠØ§Øª Ø®Ø´Ø¨ÙŠØ©': 'WOOD_FLOOR',
        'Ø¨Ø§Ø±ÙƒÙŠÙ‡': 'WOOD_FLOOR',
        'Ø£Ø³Ù‚Ù Ù…Ø¹Ù„Ù‚Ø©': 'CEIL_SUSP',
        'Ø£Ø³Ù‚Ù Ù…Ø³ØªØ¹Ø§Ø±Ø©': 'CEIL_ACOUS',
        'ØªÙ…Ø¯ÙŠØ¯Ø§Øª Ù…ÙŠØ§Ù‡': 'WATER_PIPE',
        'ØµØ±Ù ØµØ­ÙŠ': 'SANITARY',
        'ØªØ¬Ù‡ÙŠØ²Ø§Øª ØµØ­ÙŠØ©': 'FIXTURES'
    }
    
    prefix = (category_id or 'GEN').upper()[:4]
    
    for ar_term, key_suffix in key_map.items():
        if ar_term in name_ar:
            return f"{prefix}_{key_suffix}"
    
    # Fallback: generate from first few chars
    clean_name = re.sub(r'[^\w\s]', '', name_ar)[:10].upper()
    return f"{prefix}_{clean_name}"

def extract_unit(text):
    """Extract default unit from text"""
    if not text:
        return "mÂ²"
    
    unit_patterns = [
        (r'm[Â³Â³]|Ù…Â³|CUM', 'CUM'),
        (r'm[Â²Â²]|Ù…Â²|SQM', 'SQM'),
        (r'RM|LM|Ù…\.Ø·', 'RM'),
        (r'KG|TON|Ø·Ù†', 'TON'),
        (r'each|Ø¹Ø¯Ø¯|count', 'EACH'),
        (r'system|Ù†Ø¸Ø§Ù…', 'SYSTEM'),
        (r'day|ÙŠÙˆÙ…', 'DAY')
    ]
    
    for pattern, unit in unit_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return unit
    
    return "SQM"

def extract_notes(text):
    """Extract implementation notes from text"""
    if not text:
        return ""
    # Remove unit mentions and clean up
    notes = re.sub(r'm[Â²Â³]|Ù…[Â²Â³]|CUM|SQM|RM|LM', '', text)
    notes = re.sub(r'\s*[â€”â€“-]\s*', ' - ', notes)
    return notes.strip()[:200]  # Limit length

def translate_item_name(name_ar):
    """Basic translation for common terms"""
    translations = {
        'Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù†ÙØµÙ„Ø©': 'Isolated Footings',
        'Ù‚ÙˆØ§Ø¹Ø¯ Ø´Ø±ÙŠØ·ÙŠØ©': 'Strip Footings',
        'Ù„Ø¨Ø´Ø©': 'Raft Foundation',
        'Ø³Ù…Ù„Ø§Øª': 'Tie Beams',
        'ÙƒÙ…Ø±Ø§Øª Ø£Ø±Ø¶ÙŠØ©': 'Ground Beams',
        'Ø£Ø¹Ù…Ø¯Ø© Ø®Ø±Ø³Ø§Ù†ÙŠØ©': 'Concrete Columns',
        'Ø¨Ù„Ø§Ø·Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ø¨Ø©': 'Slab on Grade',
        'Ø¨Ù„Ø§Ø·Ø§Øª Ø£Ø¯ÙˆØ§Ø±': 'Suspended Slabs',
        'Ø´Ø¯Ø© Ø¹Ø§Ù…Ø©': 'General Formwork',
        'ØªØ³Ù„ÙŠØ­ Ø¹Ø§Ù…Ø©': 'General Reinforcement',
        'ØµØ¨ ÙˆØ®Ø±Ø³Ø§Ù†Ø©': 'Casting & Curing',
        'ØªØ¬Ø±ÙŠÙ': 'Topsoil Stripping',
        'Ø­ÙØ± Ù„Ù„Ù‚ÙˆØ§Ø¹Ø¯': 'Foundation Excavation',
        'Ø³Ù†Ø¯ Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„Ø­ÙØ±': 'Shoring',
        'Ø±Ø¯Ù… ÙˆØ¯Ù…Ùƒ': 'Backfill & Compaction',
        'ØªØ³ÙˆÙŠØ©': 'Grading',
        'Ù†Ø²Ø­ Ø³Ø·Ø­ÙŠ': 'Surface Drainage',
        'Ù†Ø²Ø­ Ø¬ÙˆÙÙŠ': 'Dewatering',
        'Ø´Ø¨ÙƒØ§Øª ØªØµØ±ÙŠÙ': 'Drainage Networks',
        'Ø¹Ø²Ù„ Ù‚ÙˆØ§Ø¹Ø¯': 'Foundation Waterproofing',
        'Ø¹Ø²Ù„ Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª': 'Wet Area Waterproofing',
        'Ø¹Ø²Ù„ Ø§Ù„Ø£Ø³Ø·Ø­': 'Roof Waterproofing',
        'Ø¹Ø²Ù„ Ø®Ø²Ø§Ù†Ø§Øª': 'Tank Waterproofing',
        'Ù…Ø­Ø§Ø±Ø©': 'Plastering',
        'Ø¯Ù‡Ø§Ù†Ø§Øª Ø¯Ø§Ø®Ù„ÙŠØ©': 'Interior Painting',
        'Ø¨Ù„Ø§Ø· Ø³ÙŠØ±Ø§Ù…ÙŠÙƒ': 'Ceramic Tiling',
        'Ø±Ø®Ø§Ù…': 'Marble Finishes',
        'Ø¬Ø±Ø§Ù†ÙŠØª': 'Granite Finishes',
        'Ø£Ø±Ø¶ÙŠØ§Øª Ø®Ø´Ø¨ÙŠØ©': 'Wood Flooring',
        'Ø£Ø³Ù‚Ù Ù…Ø¹Ù„Ù‚Ø©': 'Suspended Ceilings',
        'Ø£Ø³Ù‚Ù Ù…Ø³ØªØ¹Ø§Ø±Ø©': 'Acoustic Ceilings',
        'ØªÙ…Ø¯ÙŠØ¯Ø§Øª Ù…ÙŠØ§Ù‡': 'Water Supply Piping',
        'ØµØ±Ù ØµØ­ÙŠ': 'Sanitary Piping',
        'ØªØ¬Ù‡ÙŠØ²Ø§Øª ØµØ­ÙŠØ©': 'Plumbing Fixtures'
    }
    
    for ar, en in translations.items():
        if ar in name_ar:
            return en
    return name_ar

def generate_synonyms_ar(name_ar):
    """Generate Arabic synonyms"""
    synonyms = [name_ar]
    
    # Add common variations
    if 'Ù‚ÙˆØ§Ø¹Ø¯' in name_ar:
        synonyms.extend(['Ø£Ø³Ø§Ø³Ø§Øª', 'ÙÙˆØªÙŠÙ†Ø¬'])
    if 'Ù„Ø¨Ø´Ø©' in name_ar:
        synonyms.extend(['Ø­ØµÙŠØ±Ø©', 'Ø±Ø§ÙØª', 'mat foundation'])
    if 'Ù…Ø­Ø§Ø±Ø©' in name_ar:
        synonyms.extend(['Ø¨ÙŠØ§Ø¶', 'Ù„ÙŠØ§Ø³Ø©', 'plaster'])
    if 'Ø¹Ø²Ù„' in name_ar:
        synonyms.append('insulation')
    if 'Ø¯Ù‡Ø§Ù†Ø§Øª' in name_ar:
        synonyms.extend(['Ø¨ÙˆÙŠØ©', 'Ø·Ù„Ø§Ø¡', 'paint'])
    
    return list(set(synonyms))[:5]

def generate_synonyms_en(name_en):
    """Generate English synonyms"""
    synonyms = [name_en.lower()] if name_en else []
    
    name_lower = name_en.lower() if name_en else ""
    
    if 'footing' in name_lower:
        synonyms.extend(['foundation', 'base'])
    if 'raft' in name_lower:
        synonyms.extend(['mat foundation', 'slab foundation'])
    if 'plaster' in name_lower:
        synonyms.extend(['render', 'rendering'])
    if 'waterproof' in name_lower:
        synonyms.extend(['membrane', 'damp proofing'])
    
    return list(set(synonyms))[:5]

def merge_with_existing(new_items):
    """Merge new items with existing database"""
    print("\nğŸ“š Loading existing database...")
    
    try:
        with open(EXISTING_JSON, 'r', encoding='utf-8') as f:
            existing_data = json.load(f)
    except FileNotFoundError:
        existing_data = {"categories": [], "items": []}
    
    # Create lookup by item_key
    existing_keys = {item.get('item_key', ''): item for item in existing_data.get('items', [])}
    
    # Merge or add new items
    merged_items = []
    updated_count = 0
    new_count = 0
    
    for new_item in new_items:
        key = new_item['item_key']
        if key in existing_keys:
            # Merge: add new fields to existing item
            merged = existing_keys[key].copy()
            merged['typical_activities'] = new_item.get('typical_activities', [])
            merged['implementation_notes'] = new_item.get('implementation_notes', '')
            if 'csi_section' in new_item:
                merged['csi_section'] = new_item['csi_section']
            merged_items.append(merged)
            updated_count += 1
        else:
            merged_items.append(new_item)
            new_count += 1
    
    # Add remaining existing items that weren't updated
    for key, item in existing_keys.items():
        if not any(m['item_key'] == key for m in merged_items):
            merged_items.append(item)
    
    print(f"  âœ… Updated: {updated_count} items")
    print(f"  â• New: {new_count} items")
    print(f"  ğŸ“Š Total: {len(merged_items)} items")
    
    # Build final structure
    final_data = {
        "version": "2.0",
        "last_updated": "2026-01-04",
        "categories": existing_data.get('categories', []),
        "items": merged_items
    }
    
    return final_data

def main():
    print("=" * 60)
    print("ğŸš€ CSI Database Enhancement Tool")
    print("=" * 60)
    
    # Extract data from Excel
    new_items = parse_excel_data()
    print(f"\nğŸ“Š Extracted {len(new_items)} items from Excel")
    
    # Merge with existing database
    final_data = merge_with_existing(new_items)
    
    # Save enhanced database
    print(f"\nğŸ’¾ Saving to {OUTPUT_JSON}...")
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    
    print("\n" + "=" * 60)
    print("âœ… Enhancement complete!")
    print("=" * 60)
    
    # Print summary
    print("\nğŸ“‹ Sample enhanced item:")
    if final_data['items']:
        sample = final_data['items'][0]
        print(json.dumps(sample, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main()
